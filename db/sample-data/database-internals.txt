If we have a file that is rightly compacted, stored in the disk in closed byte positions, then it’s MUCH faster than if the bytes are randomly placed throughout

# Representation

Fixed size records—integers and that sort of thing.

- Suppose we have a bunch of tuples, need to find students with specific ID
- File stored with the following structure

| ID | Balance |
| --- | --- |
| 1 | 17.00 |
| 5 | 19.20 |
| 8 | 21.1 |
| 9 | 0.50 |
|  |  |
- Problem: whole thing doesn’t fit in memory
    - Solution: caching
- Problem: insertion is horrendous O(N)
- Costs are significant

## Another Way

Keep an index in memory, takes primary key and returns location in storage of the corresponding tuple

- Pro: no sorting required, constant lookup
- Con: can’t get a range easily

## Adding Blanks

Maintain list of gaps to keep track of next gap

# Variable Size Records

We might have enough space in the database, but the space might not be contiguous

- **********************external fragmentation**********************

Ideally everything would be fixed size, but in the real world people have different length names

We organize all records within a *****block*****

- Slotted page header contains the number of record entries and the end of free space in the block
- Fragmentation control needs to be enforced
    - Move records and update the headed (blocks are small so cheap)
    - Requires extra indirection until reaching the record